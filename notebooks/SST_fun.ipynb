{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_average(da,dt,**kwargs):\n",
    "    #dt specifies the type of resampling and can take values of 'M', 'Y' or 'DJF' for seasonshh\n",
    "    if len(dt)==3:\n",
    "        quarters = {'DJF':'Q-Feb','MAM':'Q-May','JJA':'Q-Aug','SON':'Q-Nov'}\n",
    "        mth_num = {'DJF':2,'MAM':5,'JJA':8,'SON':11}\n",
    "        m = mth_num[dt]\n",
    "        dt_q = quarters[dt]\n",
    "        t_unit = 'Y'\n",
    "        # Method 1: ignores incomplete seasons\n",
    "        if kwargs['ignore_inc']:\n",
    "            avg_da=da[kwargs['var']].resample(time=dt_q).mean(skipna = True)\n",
    "            avg_da = avg_da.sel(time=avg_da['time.month']==m).groupby('time.year').mean()\n",
    "        else:\n",
    "        # Method2: replaces incomplete seasons with Na\n",
    "            avg_da=da[kwargs['var']].resample(time='1M').mean()\n",
    "            avg_da = da.where(avg_da.time.dt.season==dt).rolling(min_periods=3,center=True,time=3).mean()\n",
    "            avg_da = avg_da.groupby('time.year').mean('time')[kwargs['var']]\n",
    "    else:\n",
    "        t_unit = dt\n",
    "        dt = '1'+dt\n",
    "        avg_da=da[kwargs['var']].resample(time=dt).mean(skipna=True)\n",
    "        avg_da['time'] = avg_da.time.astype('datetime64[' + t_unit +']')\n",
    "        if not kwargs['ignore_inc']:\n",
    "            ext_time = avg_da['time'][[0, len(avg_da['time'])-1]]\n",
    "            if (da.time[len(da.time)-1].dt.day.values < 15) or ((da.time[len(da.time)-1].dt.month.values < 12) and (t_unit=='Y')):\n",
    "                avg_da = avg_da.where(avg_da.time!=avg_da.time[len(avg_da.time)-1])\n",
    "\n",
    "            if avg_da['time'][0] < da['time'][0]-pd.to_timedelta(15,'D'):\n",
    "                avg_da = avg_da.where(avg_da.time!=avg_da.time[0])\n",
    "            #last_day = da.time[(da.time + pd.to_timedelta(1,'D')).dt.month != (da.time).dt.month].astype('datetime64[M]') \n",
    "            #avg_da = avg_da.where((avg_da.time.isin(last_day) & avg_da.time.isin(da.time)))\n",
    "\n",
    "    return avg_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_excel(df_list,file_name = \"myfile\",*args):\n",
    "# args can contain a list of the name of sheets (sheetname)\n",
    "# large spatial daily files can take 10-20mins\n",
    "    if 'sheetname' in args:\n",
    "        sheetname = args[\"sheet_name\"]\n",
    "    else:\n",
    "        sheetname = list(str(range(1,len(df_list)+1)))\n",
    "        \n",
    "    with pd.ExcelWriter(file_name+'.xlsx') as writer:\n",
    "    \n",
    "        for i in range(0,len(df_list)):\n",
    "            if df_list[i].shape[0]>=2**20:\n",
    "                print(\"Warning: data #\" +str(i)+ \" is too large to save as a .xlsx file\")\n",
    "            else:\n",
    "                df_list[i].to_excel(writer,sheet_name=str(sheetname[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clim(da,time_res='month',**kwargs):\n",
    "    if 'time_slice' in kwargs:\n",
    "        ct = da.sel(time=slice(kwargs['time_slice'][0],kwargs['time_slice'][1])).groupby('time.' + time_res).count(dim='time')\n",
    "        s = da.sel(time=slice(kwargs['time_slice'][0],kwargs['time_slice'][1])).groupby('time.' + time_res).std(dim='time')\n",
    "        clim = da.sel(time=slice(kwargs['time_slice'][0],kwargs['time_slice'][1])).groupby('time.' + time_res).mean(dim='time')\n",
    "    else:\n",
    "        clim = da.groupby('time.' + time_res).mean(dim='time')\n",
    "        s = da.groupby('time.' + time_res).std(dim='time')\n",
    "        ct = da.groupby('time.' + time_res).count(dim='time')\n",
    "        \n",
    "    h95 = clim + 1.96*s/np.sqrt(ct)\n",
    "    l95 = clim - 1.96*s/np.sqrt(ct)\n",
    "    return clim,h95,l95\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_trend(da,coord,deg=1):\n",
    "    \n",
    "    f=da.polyfit(dim=coord,deg=1)\n",
    "    fit = xr.polyval(da[coord],f)\n",
    "    fit = fit.rename({'polyfit_coefficients':'linear_fit'})\n",
    "    n = len(da[coord])\n",
    "    x2 = xr.DataArray(range(1,len(da[coord])+1),dims =coord,coords={coord:da[coord]})\n",
    "    serr= np.sqrt(((da-fit['linear_fit'])**2).sum(dim=coord)/(len(da[coord])-1)).expand_dims(dim = {coord:n})\n",
    "    t = stats.t.ppf(1-0.025, len(da[coord]))\n",
    "    B = np.sqrt(1/n + (x2 - np.mean(x2))**2 / np.sum((x2-np.mean(x2))**2))\n",
    "    ci = B*serr*t\n",
    "    ci+fit\n",
    "\n",
    "    hci = ci+fit\n",
    "    lci = fit-ci\n",
    "   # f=da.polyfit(dim=coord,deg=1)\n",
    "   # fit = xr.polyval(da[coord],f)\n",
    "   # fit = fit.rename({'polyfit_coefficients':'linear_fit'})\n",
    "   # n = len(da[coord])\n",
    "   # x2 = range(1,len(da[coord])+1)\n",
    "   # serr = np.sqrt(np.sum((da-fit)**2)/(len(da[coord])-1))\n",
    "   # t = stats.t.ppf(1-0.025, len(da[coord]))\n",
    "\n",
    "    #ci = t * serr['linear_fit'].values * np.sqrt(1/n + (x2 - np.mean(x2))**2 / np.sum((x2-np.mean(x2))**2))\n",
    "    #hci = ci+fit\n",
    "    #lci = fit-ci\n",
    "    return f,fit,hci,lci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clim_plot(da,time_main,time_res,**kwargs):\n",
    "    col_yr = ['red', 'blue', 'green', 'yellow', 'purple'] # this could be extended for more years\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    clim,h95,l95 = make_clim(da,time_res,time_slice=time_main)\n",
    "    clim.plot(label='Clim',color = 'black')\n",
    "    plt.fill_between(h95[time_res],l95,h95,alpha=0.5,color='grey')\n",
    "    if 'time_recent' in kwargs:\n",
    "        tt = make_clim(da,time_res,time_slice= kwargs['time_recent'])[0].plot(label='Recent',color = 'black',linestyle='dashed')\n",
    "    if 'ind_yr' in kwargs:\n",
    "        ind_yr = kwargs['ind_yr']\n",
    "        i=0\n",
    "        for y in kwargs['ind_yr']:\n",
    "            time_ind_yr = [str(y) +'-01-01', str(y) +'-12-31']\n",
    "            make_clim(da,time_res,time_slice=time_ind_yr)[0].plot(label=str(y),color = col_yr[i])\n",
    "            i+=1\n",
    "            \n",
    "    xl = clim.coords[time_res].values\n",
    "    plt.xlim(xl.min(),xl.max())\n",
    "    if time_res == 'month':\n",
    "        ax.set_xticks([2,4,6,8,10,12])\n",
    "        tlab = [datetime.date(1990,x,1).strftime('%b') for x in ax.get_xticks().astype(int) if x > 0 and x <13]\n",
    "        ax.set_xticklabels([datetime.date(1990,x,1).strftime('%b') for x in [2,4,6,8,10,12]])\n",
    "    plt.legend(loc = 'upper right')\n",
    "    return clim,ax\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## correct mapping\n",
    "def map(da,lim_lon,lim_lat,cmap,title = 'Map',subpos = 111,**map_kwargs):\n",
    "    \n",
    "    axpl = da.plot.contourf(transform = ccrs.PlateCarree(),x = 'lon',y='lat',extend='both',cmap=cmap,subplot_kws={'projection':ccrs.PlateCarree()},**map_kwargs)\n",
    "    ax = plt.gca()\n",
    "    gl = plt.gca().gridlines(draw_labels=True)\n",
    "    gl.top_labels=False\n",
    "    gl.right_labels=False\n",
    "    ax.add_feature(cartopy.feature.LAND,color = 'grey',edgecolor = 'black')\n",
    "    ax.set_extent(lim_lon+lim_lat)\n",
    "    ax.set_title(title,fontsize = 20);\n",
    "    return ax,gl,axpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be called if no existing cb\n",
    "def create_cb(fig,ax,ax_proj,label = '',size = \"4%\", pad = 0.5,**kwargs):\n",
    "    \n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    divider = make_axes_locatable(ax)\n",
    "    ax_cb = divider.new_horizontal(size = size, pad = pad, axes_class = plt.Axes)\n",
    "    fig.add_axes(ax_cb)\n",
    "    cb = plt.colorbar(ax_proj,cax = ax_cb)\n",
    "    cb.set_label(label,**kwargs)\n",
    "    return cb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_and_crop(fo,\n",
    "                  storage_options=dict(anon=True),\n",
    "                  data_vars=['sea_surface_temperature','l2p_flags'],\n",
    "                  cropto=dict(lat=slice(-32.0,-32.5),lon=slice(115.0,115.5)),\n",
    "                  load=True,\n",
    "                  simple_templates=True):\n",
    "    \n",
    "    mapper=fsspec.get_mapper('reference://',\n",
    "                         fo=fo,\n",
    "                         target_options=storage_options,\n",
    "                         remote_protocol='s3',\n",
    "                         remote_options=storage_options,\n",
    "                        )\n",
    "    ds = xr.open_zarr(mapper,chunks={'time':14})   \n",
    "\n",
    "    ds = ds[data_vars]\n",
    "    ds = ds.sel(**cropto)\n",
    "    \n",
    "    #Loading the data at this point causes issues with dask, due to task nesting - works OK using single-threaded, which makes sense\n",
    "    if load:\n",
    "        import dask\n",
    "        with dask.config.set(scheduler='single-threaded'):\n",
    "            try:\n",
    "                ds = ds.load() #loads this months data\n",
    "            except Exception as ex:\n",
    "                print('Load Failed: ' + fo)\n",
    "                return None\n",
    "    \n",
    "    return ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_stack(s3_path,\n",
    "               extension='json',\n",
    "               storage_options=dict(anon=True),\n",
    "               open_func=open_and_crop,\n",
    "               open_kwargs={}):\n",
    "    \n",
    "    # Get the list of reference files\n",
    "    fs = fsspec.filesystem('s3',**storage_options)\n",
    "    agg_files = fs.glob(f'{s3_path}/*.{extension}')\n",
    "    \n",
    "    open_kwargs['storage_options']=storage_options\n",
    "    \n",
    "    from dask import compute, delayed\n",
    "    d_open_dataset = delayed(open_func)\n",
    "    futures = []\n",
    "    for f in agg_files:\n",
    "        futures.append(d_open_dataset('s3://' + f,**open_kwargs))\n",
    "    dsets = compute(futures)[0]\n",
    "    \n",
    "    for i,ds in enumerate(dsets):\n",
    "        if ds is None:\n",
    "            print('Failed to open: ' + agg_files[i])\n",
    "            \n",
    "    dsets = [ds for ds in dsets if ds is not None]\n",
    "    \n",
    "    xarray_concat_kwargs = dict(dim='time',coords='minimal',join='override',compat='override',combine_attrs='override')\n",
    "    ds = xr.concat(dsets, **xarray_concat_kwargs)\n",
    "    ds = ds.sortby('time') # The sort is beneficial to re-order the data according to the coordinate, as having separate stacks for the different layouts breaks the ordering.\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_65/3739016906.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msecret\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msecret\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_creds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_65/3739016906.py\u001b[0m in \u001b[0;36mload_creds\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load credentials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_creds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'HOME'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/.aws/credentials.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Load credentials\n",
    "def load_creds():\n",
    "    with open(os.environ['HOME'] + '/.aws/credentials','rt') as f:\n",
    "        f.readline()\n",
    "        key=f.readline().split('=')[1].strip()\n",
    "        secret=f.readline().split('=')[1].strip()\n",
    "    return key, secret\n",
    "key,secret=load_creds()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
